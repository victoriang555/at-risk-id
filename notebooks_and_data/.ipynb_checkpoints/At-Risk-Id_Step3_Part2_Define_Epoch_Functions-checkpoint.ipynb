{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/victoria/anaconda3/envs/ds/lib/python3.6/site-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
      "  \"\"\")\n",
      "2018-06-03 20:01:04,484 : INFO : 'pattern' package not found; tag filters are not available for English\n",
      "/home/victoria/anaconda3/envs/ds/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# General system libraries\n",
    "import os\n",
    "import sys\n",
    "from IPython.display import Image, Markdown\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# Dataframe libraries\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, read_csv\n",
    "\n",
    "# Number manipulation\n",
    "import scipy.sparse\n",
    "from scipy.ndimage.filters import generic_filter\n",
    "import patsy\n",
    "import numpy as np\n",
    "\n",
    "# Plotting libaries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "\n",
    "# Data type libaries\n",
    "from datetime import datetime as dt\n",
    "\n",
    "# File manipulation\n",
    "import pickle\n",
    "import pandas.io.sql as pd_sql\n",
    "from sqlalchemy import create_engine\n",
    "import psycopg2 as pg\n",
    "from flatten_json import flatten\n",
    "\n",
    "# NLP libraries\n",
    "import wikipedia as wiki\n",
    "from nltk import word_tokenize, sent_tokenize,FreqDist, pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "import gensim as gn\n",
    "from gensim import corpora, models, similarities\n",
    "from collections import defaultdict\n",
    "from six import iteritems\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import string\n",
    "import emoji\n",
    "import enchant\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "# Scraping libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from scraping_functions.tumblr_api import get_client\n",
    "import pytumblr\n",
    "\n",
    "# Stats libaries\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn import datasets, linear_model, metrics\n",
    "from sklearn.cross_validation import cross_val_score, cross_val_predict\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import Ridge, Lasso, RidgeCV, LinearRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import PCA, NMF, TruncatedSVD, LatentDirichletAllocation\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "\n",
    "# Other libaries\n",
    "import geopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import all corpuses df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_corpuses_df = pd.read_pickle('iteration1_files/all_corpuses_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>stemmed_text</th>\n",
       "      <th>lemmatized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>spaceeblack</td>\n",
       "      <td>Space Boy</td>\n",
       "      <td>space boy</td>\n",
       "      <td>space boy</td>\n",
       "      <td>space boy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>vongriffis</td>\n",
       "      <td>seen it all before // bring me the horizon</td>\n",
       "      <td>seen it all before  bring me the horizon</td>\n",
       "      <td>seen it all befor bring me the horizon</td>\n",
       "      <td>see it all before bring me the horizon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>vongriffis</td>\n",
       "      <td>$UICIDEBOY$ // Kill Yourself (Part III)</td>\n",
       "      <td>uicideboy  kill yourself part iii</td>\n",
       "      <td>uicideboy kill yourself part iii</td>\n",
       "      <td>uicideboy kill yourself part iii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>vongriffis</td>\n",
       "      <td>I feel like I’m a no one, that’s what they tol...</td>\n",
       "      <td>i feel like im a no one thats what they told me</td>\n",
       "      <td>i feel like im a no one that what they told me</td>\n",
       "      <td>i feel like im a no one thats what they tell me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>vongriffis</td>\n",
       "      <td>Issues - The Worst Of Them</td>\n",
       "      <td>issues  the worst of them</td>\n",
       "      <td>issu the worst of them</td>\n",
       "      <td>issue the worst of them</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       username                                               text  \\\n",
       "19  spaceeblack                                          Space Boy   \n",
       "22   vongriffis         seen it all before // bring me the horizon   \n",
       "26   vongriffis            $UICIDEBOY$ // Kill Yourself (Part III)   \n",
       "30   vongriffis  I feel like I’m a no one, that’s what they tol...   \n",
       "31   vongriffis                         Issues - The Worst Of Them   \n",
       "\n",
       "                                       cleaned_text  \\\n",
       "19                                        space boy   \n",
       "22         seen it all before  bring me the horizon   \n",
       "26                uicideboy  kill yourself part iii   \n",
       "30  i feel like im a no one thats what they told me   \n",
       "31                        issues  the worst of them   \n",
       "\n",
       "                                      stemmed_text  \\\n",
       "19                                       space boy   \n",
       "22          seen it all befor bring me the horizon   \n",
       "26                uicideboy kill yourself part iii   \n",
       "30  i feel like im a no one that what they told me   \n",
       "31                          issu the worst of them   \n",
       "\n",
       "                                    lemmatized_text  \n",
       "19                                        space boy  \n",
       "22           see it all before bring me the horizon  \n",
       "26                 uicideboy kill yourself part iii  \n",
       "30  i feel like im a no one thats what they tell me  \n",
       "31                          issue the worst of them  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_corpuses_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8771, 5)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_corpuses_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a function to Vectorize the text both ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_both_ways(cleaned_df, text_to_vectorize):\n",
    "    stop_words = list(STOP_WORDS)\n",
    "    stop_words.append('a')\n",
    "    cv = CountVectorizer(stop_words=stop_words)\n",
    "    tfidf = TfidfVectorizer(stop_words=stop_words)\n",
    "    corpus = cleaned_df[text_to_vectorize]\n",
    "    cv_fitted = cv.fit(corpus)\n",
    "    tfidf_fitted = tfidf.fit(corpus)\n",
    "    cv_data = cv.fit_transform(corpus)\n",
    "    tfidf_data = tfidf.fit_transform(corpus)\n",
    "    return cv_fitted, cv_data, tfidf_fitted, tfidf_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_fitted_test, cv_data_test, tfidf_fitted_test, tfidf_data_test = vectorize_both_ways(all_corpuses_df[:100], 'cleaned_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None,\n",
       "        stop_words=['yours', 'wherein', 'say', 'had', 'someone', 'elsewhere', 'do', 'go', 'none', 'besides', 'yourselves', 'below', 'therein', 'everything', 'always', 'mine', 'well', 'about', 'beside', 'while', 'noone', 're', 'show', 'her', 'nine', 'does', 'never', 'ca', 'i', 'off', 'if', 'ever', 'whatever'...', 'who', 'an', 'amongst', 'anyhow', 'becomes', 'one', 'but', 'become', 'nevertheless', 'down', 'a'],\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_fitted_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<100x438 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 552 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=['yours', 'wherein', 'say', 'had', 'someone', 'elsewhere', 'do', 'go', 'none', 'besides', 'yourselves', 'below', 'therein', 'everything', 'always', 'mine', 'well', 'about', 'beside', 'while', 'noone', 're', 'show', 'her', 'nine', 'does', 'never', 'ca', 'i', 'off', 'if', 'ever', 'whatever'...', 'who', 'an', 'amongst', 'anyhow', 'becomes', 'one', 'but', 'become', 'nevertheless', 'down', 'a'],\n",
       "        strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_fitted_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<100x438 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 552 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_data_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_fitted, cv_data, tfidf_fitted, tfidf_data = vectorize_both_ways(all_corpuses_df, 'cleaned_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(cv_fitted, open('iteration1_files/cv_fitted', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(cv_data, open('iteration1_files/cv_data', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(tfidf_fitted, open('iteration1_files/tfidf_fitted', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(tfidf_data, open('iteration1_files/tfidf_data', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a function to generate fitted vectorization/model combos and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_vectorizer_model_combos(cv_fitted, cv_data, tfidf_fitted, tfidf_data, n_topics=5, random_state=30):\n",
    "    n_topics=n_topics\n",
    "    random_state=random_state\n",
    "    \n",
    "    \n",
    "    nmf_cv = NMF(n_components=n_topics, random_state=random_state)\n",
    "    nmf_cv_data = nmf_cv.fit_transform(cv_data)\n",
    "    \n",
    "    nmf_tfidf = NMF(n_components=n_topics, random_state=random_state)\n",
    "    nmf_tfidf_data = nmf_tfidf.fit_transform(tfidf_data)\n",
    "    \n",
    "    lsa_cv = TruncatedSVD(n_components=n_topics, random_state=random_state)\n",
    "    lsa_cv_data = lsa_cv.fit_transform(cv_data)\n",
    "    \n",
    "    lsa_tfidf = TruncatedSVD(n_components=n_topics, random_state=random_state)\n",
    "    lsa_tfidf_data = lsa_tfidf.fit_transform(tfidf_data)\n",
    "    \n",
    "    lda_cv = LatentDirichletAllocation(n_components=n_topics, random_state=random_state)\n",
    "    lda_cv_data = lda_cv.fit_transform(cv_data)\n",
    "    \n",
    "    lda_tfidf = LatentDirichletAllocation(n_components=n_topics, random_state=random_state)\n",
    "    lda_tfidf_data = lda_tfidf.fit_transform(tfidf_data)\n",
    "    \n",
    "    combo_models_list = [nmf_cv, nmf_tfidf, lsa_cv, lsa_tfidf, lda_cv, lda_tfidf]\n",
    "    \n",
    "    return nmf_cv, nmf_cv_data, nmf_tfidf, nmf_tfidf_data, lsa_cv, lsa_cv_data, lsa_tfidf, lsa_tfidf_data, lda_cv, lda_cv_data, lda_tfidf, lda_tfidf_data, combo_models_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/victoria/anaconda3/envs/ds/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/victoria/anaconda3/envs/ds/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "nmf_cv, nmf_cv_data, nmf_tfidf, nmf_tfidf_data, lsa_cv, lsa_cv_data, lsa_tfidf, lsa_tfidf_data, lda_cv, lda_cv_data, lda_tfidf, lda_tfidf_data, combo_models_list = gen_vectorizer_model_combos(cv_fitted, cv_data, tfidf_fitted, tfidf_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(nmf_cv, open('iteration1_files/nmf_cv.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(nmf_cv_data, open('iteration1_files/nmf_cv_data.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(nmf_tfidf, open('iteration1_files/nmf_tfidf.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(nmf_tfidf_data, open('iteration1_files/nmf_tfidf_data.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(lsa_cv, open('iteration1_files/lsa_cv.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(lsa_cv_data, open('iteration1_files/lsa_cv_data.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(lsa_tfidf, open('iteration1_files/lsa_tfidf.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(lsa_tfidf_data, open('iteration1_files/lsa_tfidf_data.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(lda_cv, open('iteration1_files/lda_cv.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(lda_cv_data, open('iteration1_files/lda_cv_data.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(lda_tfidf, open('iteration1_files/lda_tfidf.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(lda_tfidf_data, open('iteration1_files/lda_tfidf_data.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(combo_models_list, open('iteration1_files/combo_models_list.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a function to obtain a topics df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_topics_for_one_combo(combo_model, combo_model_name, fitted_vectorizer, num_top_words):\n",
    "    feature_names = fitted_vectorizer.get_feature_names()\n",
    "    \n",
    "    for idx, topic in enumerate(combo_model.components_):\n",
    "        topic_words = \" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-num_top_words - 1:-1]])\n",
    "        return \"{}_Topic{}: {}\".format(combo_model_name, idx+1, topic_words)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lda_tfidf_Topic1: weight trying okay httpiglovequotesnet thoughts lets week head kill tips meet wiltedflower relate alienmonster dropneurons yall lose photos psa goal'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_topics_for_one_combo(lda_tfidf, 'lda_tfidf', tfidf_fitted, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lda_cv_Topic1: weight reblog okay stay trying guys lose black world said year try white hard kill head skinny words calories cut'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_topics_for_one_combo(lda_cv, 'lda_cv', cv_fitted, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_topics_df(combo_models_list, cv_fitted, tfidf_fitted, num_top_words):\n",
    "    for combo_model in combo_models_list:\n",
    "        gen_topics_for_one_combo(combo_model)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        print(\"Topic \", ix+1)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic  1\n",
      "weight trying okay httpiglovequotesnet thoughts lets week head kill tips meet wiltedflower relate alienmonster dropneurons yall lose photos psa goal\n",
      "Topic  2\n",
      "im happy look things follow month good pride blog need today wish sorry dark like love cute thats redheart pretty\n",
      "Topic  3\n",
      "cant im dont want love hate life like feel friends people lost way help oh right reblog sad tell ive\n",
      "Topic  4\n",
      "weheartit love hey thinspo mood httpswwwinstagramcomthepersonalquotes care black night ig pics white great time kiss sleep fall home use money\n",
      "Topic  5\n",
      "want like know dont people time youre im love die fuck day new beautiful edit hard need art stop post\n"
     ]
    }
   ],
   "source": [
    "display_topics(lda_tfidf, tfidf_fitted.get_feature_names(), 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'get_feature_names'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-3ab465005a22>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlsa_tfidf_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'get_feature_names'"
     ]
    }
   ],
   "source": [
    "lsa_tfidf_data.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa_tfidf.fe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lda' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-870d245e010c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m                         for i in topic.argsort()[:-no_top_words - 1:-1]]))\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdisplay_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcount_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'lda' is not defined"
     ]
    }
   ],
   "source": [
    "display_topics(lda,count_vectorizer.get_feature_names(),20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topics_df(nmf_cv, ):\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
